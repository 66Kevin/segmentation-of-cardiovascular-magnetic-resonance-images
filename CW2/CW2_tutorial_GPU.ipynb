{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2guthgOz-Ssm"
   },
   "source": [
    "# Coursework 2 for Cardiac MR Image Segmentation (2020-2021)\n",
    "\n",
    "After you have gone through the coursework description, this tutorial is designed to further helps you understand the problem and therefore enable you to propose a good solution for this coursework. You will learn:\n",
    "\n",
    "* how to load and save images with OpenCV\n",
    "* how to train a segmentation model with Pytorch\n",
    "* how to evaluate the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsnVbP35-Sso"
   },
   "source": [
    "## 1. Load, show, and save images with OpenCV\n",
    "\n",
    "OpenCV is an open-source computer vision library which helps us to manipulate image data. In this section, we will cover:\n",
    "* Loading an image from file with imread()\n",
    "* Displaying the image with matplotlib plt.imshow()\n",
    "* Saving an image with imwrite()\n",
    "\n",
    "For a more comprehensive study of OpenCV, we encourage you to check the official [OpenCV documentation](https://docs.opencv.org/master/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "C7ZvSiY3qW_U"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def show_image_mask(img, mask, cmap='gray'): # visualisation\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img, cmap=cmap)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, cmap=cmap)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "EN5WJ_XG-Sso"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2 #import OpenCV\n",
    "\n",
    "data_dir = './data/train'\n",
    "image = cv2.imread(os.path.join(data_dir,'image','cmr1.png'), cv2.IMREAD_UNCHANGED)\n",
    "mask = cv2.imread(os.path.join(data_dir,'mask','cmr1_mask.png'), cv2.IMREAD_UNCHANGED)\n",
    "show_image_mask(image, mask, cmap='gray')\n",
    "plt.pause(1)\n",
    "cv2.imwrite(os.path.join('./','cmr1.png'), mask*85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UDvsGZnYfHS"
   },
   "source": [
    "Note: You will no doubt notice that the mask images appear to be completely black with no sign of any segmentations. This is because the max intensity of pixels in an 8-bit png image is 255 and your image viewer software only sees 255 as white. For those values close to zero, you will only see dark values. This is the case for our masks as the background, the right ventricle, the myocardium, and the left ventricle in each image are 0, 1, 2, and 3, respectively. All of which are close to zero. If we multiply the original mask by 85 and save the result to the directory where this code is, we can see the heart indeed shows up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hULAX3WH-Sss"
   },
   "source": [
    "## 2 Define a segmentation model with Pytorch\n",
    "\n",
    "In this section, we expect you to learn how to:\n",
    "* Define a Segmentation Model\n",
    "* Define a DataLoader that inputs images to the Model\n",
    "* Define training parameters and train the model\n",
    "* Test the trained model with a new input image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrKFgoZvUbeg"
   },
   "source": [
    "### 2.1 Define a DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kC9s43MqqW_U"
   },
   "source": [
    "Below we provide you with a dataloader to use in your assigment. You will only need to focus on the development of your model and loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "XYrD95T8qz8T"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "class TrainDataset(data.Dataset):\n",
    "    def __init__(self, root=''):\n",
    "        super(TrainDataset, self).__init__()\n",
    "        self.img_files = glob(os.path.join(root,'image','*.png'))\n",
    "        self.mask_files = []\n",
    "        for img_path in self.img_files:\n",
    "            basename = os.path.basename(img_path)\n",
    "            self.mask_files.append(os.path.join(root,'mask',basename[:-4]+'_mask.png'))\n",
    "            \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            mask_path = self.mask_files[index]\n",
    "            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            label = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "            return torch.from_numpy(data).float(), torch.from_numpy(label).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, root=''):\n",
    "        super(TestDataset, self).__init__()\n",
    "        self.img_files = glob(os.path.join(root,'image','*.png'))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            img_path = self.img_files[index]\n",
    "            data = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "            return torch.from_numpy(data).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82UAfnwSUgc_"
   },
   "source": [
    "### 2.2 Define a Segmenatation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEIkCqdfYnIn"
   },
   "source": [
    "You will need to define your CNN model for segmentation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "-W6532hFXa_g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "CNNSEG(\n",
      "  (encoder1): CNNblock(\n",
      "    (double_conv): Sequential(\n",
      "      (0): Conv2d(1, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (encoder2): CNNblock(\n",
      "    (double_conv): Sequential(\n",
      "      (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (encoder3): CNNblock(\n",
      "    (double_conv): Sequential(\n",
      "      (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (encoder4): CNNblock(\n",
      "    (double_conv): Sequential(\n",
      "      (0): Conv2d(384, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bottleneck): CNNblock(\n",
      "    (double_conv): Sequential(\n",
      "      (0): Conv2d(768, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (upconv4): ConvTranspose2d(1536, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder4): CNNblock(\n",
      "    (double_conv): Sequential(\n",
      "      (0): Conv2d(1536, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (upconv3): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder3): CNNblock(\n",
      "    (double_conv): Sequential(\n",
      "      (0): Conv2d(768, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (upconv2): ConvTranspose2d(384, 192, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder2): CNNblock(\n",
      "    (double_conv): Sequential(\n",
      "      (0): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (upconv1): ConvTranspose2d(192, 96, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder1): CNNblock(\n",
      "    (double_conv): Sequential(\n",
      "      (0): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (conv): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CNNblock(nn.Module):\n",
    "    def __init__(self, in_channels, features):\n",
    "        super(CNNblock, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels,out_channels=features,kernel_size=3,padding=1,bias=False,),\n",
    "            nn.BatchNorm2d(num_features=features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=features,out_channels=features,kernel_size=3,padding=1,bias=False,),\n",
    "            nn.BatchNorm2d(num_features=features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class CNNSEG(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=1, out_channels=4, features_size=96, debug=True):\n",
    "        super(CNNSEG, self).__init__()\n",
    "\n",
    "        features = features_size\n",
    "        self.debug = debug\n",
    "        self.encoder1 = CNNblock(in_channels, features)#[1,96,96,96]\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = CNNblock(features, features*2)#[1,192,48,48]\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = CNNblock(features*2, features*4)#[1,384,24,24]\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = CNNblock(features*4, features*8)#[1,768,12,12]\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck = CNNblock(features*8, features*16)#[1,1536,6,6]\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(features*16, features*8, kernel_size=2, stride=2)\n",
    "        self.decoder4 = CNNblock(features*16, features*8)#[1,768,12,12]\n",
    "        self.upconv3 = nn.ConvTranspose2d(features*8, features*4, kernel_size=2, stride=2)\n",
    "        self.decoder3 = CNNblock(features*8, features*4)#[1,384,24,24]\n",
    "        self.upconv2 = nn.ConvTranspose2d(features*4, features*2, kernel_size=2, stride=2)\n",
    "        self.decoder2 = CNNblock(features*4, features*2)#[1,192,48,48]\n",
    "        self.upconv1 = nn.ConvTranspose2d(features*2, features, kernel_size=2, stride=2)\n",
    "        self.decoder1 = CNNblock(features*2, features) #[1,96,96,96]\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=features, out_channels=out_channels, kernel_size=1)\n",
    "                                                         #[1,4,96,96]\n",
    "                                                                   \n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        pool1 = self.pool1(enc1)\n",
    "        enc2 = self.encoder2(pool1)\n",
    "        pool2 = self.pool2(enc2)\n",
    "        enc3 = self.encoder3(pool2)\n",
    "        pool3 = self.pool3(enc3)\n",
    "        enc4 = self.encoder4(pool3)\n",
    "        pool4 = self.pool4(enc4)\n",
    "\n",
    "        bottleneck = self.bottleneck(pool4)\n",
    "\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = self.decoder4(torch.cat((dec4, enc4), dim=1))\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = self.decoder3(torch.cat((dec3, enc3), dim=1))\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = self.decoder2(torch.cat((dec2, enc2), dim=1))\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = self.decoder1(torch.cat((dec1, enc1), dim=1))\n",
    "\n",
    "        if self.debug:\n",
    "            print('encode1:', enc1.size())\n",
    "            print('encode2:', enc2.size())\n",
    "            print('encode3:', enc3.size())\n",
    "            print('encode4:', enc4.size())\n",
    "            print('bottleneck:', bottleneck.size())\n",
    "            print('decode4:', dec4.size())\n",
    "            print('decode3:', dec3.size())\n",
    "            print('decode2:', dec2.size())\n",
    "            print('decode1:', dec1.size())\n",
    "            print(self.conv(dec1).size())\n",
    "\n",
    "        return torch.sigmoid(self.conv(dec1))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNNSEG().to(device)\n",
    "\n",
    "print(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRdPFTa9a34J"
   },
   "source": [
    "### 2.3 Define a Loss function and optimizer\n",
    "\n",
    "You will need to define a loss function and an optimizer. torch.nn has a variety of readymade loss functions, although you may wish to create your own instead. torch.optim has a variety of optimizers, it is advised that you use one of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "QRjOZGXRbUFT"
   },
   "outputs": [],
   "source": [
    "Loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grDz3fR1qW_V"
   },
   "source": [
    "### 2.4 Training\n",
    "\n",
    "As most of you will use CPUs to train the model, expect your models to take **30 minutes to train if not longer depending on network architecture**. To save time, you should not be using all training data until your model is well developed. If you are running your model on a GPU training should be significantly faster. During the training process, you may want to save the checkpoints as follows:\n",
    "\n",
    "```\n",
    "# Saving checkpoints for validation/testing\n",
    "torch.save(model.state_dict(), path)\n",
    "```\n",
    "The saved checkpoints can be used to load at a later date for validation and testing. Here we give some example code for training a model. Note that you need to specify the max iterations you want to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "iCb4bxVVchxf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/50 :: Ave_train_loss:1.4302\n",
      "Epoch training time:134.2391\n",
      "----------\n",
      "val_loss:1.3933\n",
      "save the best model!\n",
      "val_loss:1.3921\n",
      "save the best model!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-bb210cd3ab5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Write your FORWARD below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Then write your BACKWARD & OPTIMIZE below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-4567edd9efe2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mbottleneck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbottleneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mdec4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupconv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottleneck\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mdec4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mdec3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    927\u001b[0m         return F.conv_transpose2d(\n\u001b[1;32m    928\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def convert_to_4_channel(img):\n",
    "    converted_img = torch.zeros((img.shape[0], 4, img.shape[1], img.shape[2]))\n",
    "    # Binarization 二值化操作 使mask更清晰\n",
    "    converted_img[:, 0, :, :] = torch.where(img == 0, 1, 0)\n",
    "    converted_img[:, 1, :, :] = torch.where(img == 1, 1, 0)\n",
    "    converted_img[:, 2, :, :] = torch.where(img == 2, 1, 0)\n",
    "    converted_img[:, 3, :, :] = torch.where(img == 3, 1, 0)\n",
    "    return converted_img\n",
    "\n",
    "#################################################### Load dataset#####################################################\n",
    "\n",
    "train_data_path = './data/train'\n",
    "val_data_path = './data/val'\n",
    "num_workers = 4\n",
    "batch_size = 10\n",
    "train_set = TrainDataset(train_data_path)\n",
    "training_data_loader = DataLoader(dataset=train_set, num_workers=0, batch_size=batch_size, shuffle=True)\n",
    "val_set = TrainDataset(val_data_path)\n",
    "val_data_loader = DataLoader(dataset=val_set, num_workers=num_workers, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "###################################################### Training #######################################################\n",
    "train_loss=[]\n",
    "loss_training_epoch=[]\n",
    "best_training_loss = float(\"inf\")\n",
    "epoches = 50\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    model.train()\n",
    "    since = time.time()\n",
    "    epoch_train_loss=[]\n",
    "    \n",
    "    # Fetch images and labels.  \n",
    "    for iteration, sample in enumerate(training_data_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # The shape of img and mask is [Batch_size, 96, 96]\n",
    "        img, mask = sample\n",
    "        \n",
    "        img, mask = img.to(device), mask.to(device)\n",
    "        \n",
    "        # Adding a dimension to img:[batch_size,1,96,96], ready to feed into network\n",
    "        img = img.unsqueeze(1)\n",
    "        \n",
    "        # Calculate the expected mask\n",
    "        y_true = convert_to_4_channel(mask)\n",
    "\n",
    "        # Write your FORWARD below\n",
    "        y_pred = model(img)\n",
    "\n",
    "        # Then write your BACKWARD & OPTIMIZE below\n",
    "        loss = Loss(y_pred.float(), mask.long())\n",
    "        train_loss.append(loss.item())\n",
    "        epoch_train_loss.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "    print('Epoch:{}/{} :: Ave_train_loss:{:.4f}'.format(epoch+1,epoches,np.mean(epoch_train_loss)))\n",
    "    print('Epoch training time:{:.4f}'.format(time.time()-since))\n",
    "    print('-'*10)\n",
    "    plt.plot(train_loss)\n",
    "    loss_training_epoch.append(np.mean(epoch_train_loss))\n",
    "    \n",
    "    # At the end of the epoch, do a test on the validation set and save the best model\n",
    "    if(loss_training_epoch[-1] < best_training_loss):\n",
    "        best_training_loss = loss_training_epoch[-1]\n",
    "        best_model = model\n",
    "        torch.save(best_model.state_dict(), 'best_model.pth')\n",
    "        print('save the best model!')\n",
    "    \n",
    "torch.save(model.state_dict(), \"last_model.pth\")\n",
    "print(len(loss_training_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCZP-xof-Sst"
   },
   "source": [
    "### 2.5 Testing\n",
    "\n",
    "When validating the trained checkpoints (models), remember to change the model status as **Evaluation Mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "lGmhTdkciDt0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "LVS22lrjqW_V"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CNNSEG:\n\tMissing key(s) in state_dict: \"encoder1.double_conv.0.weight\", \"encoder1.double_conv.1.weight\", \"encoder1.double_conv.1.bias\", \"encoder1.double_conv.1.running_mean\", \"encoder1.double_conv.1.running_var\", \"encoder1.double_conv.3.weight\", \"encoder1.double_conv.4.weight\", \"encoder1.double_conv.4.bias\", \"encoder1.double_conv.4.running_mean\", \"encoder1.double_conv.4.running_var\", \"encoder2.double_conv.0.weight\", \"encoder2.double_conv.1.weight\", \"encoder2.double_conv.1.bias\", \"encoder2.double_conv.1.running_mean\", \"encoder2.double_conv.1.running_var\", \"encoder2.double_conv.3.weight\", \"encoder2.double_conv.4.weight\", \"encoder2.double_conv.4.bias\", \"encoder2.double_conv.4.running_mean\", \"encoder2.double_conv.4.running_var\", \"encoder3.double_conv.0.weight\", \"encoder3.double_conv.1.weight\", \"encoder3.double_conv.1.bias\", \"encoder3.double_conv.1.running_mean\", \"encoder3.double_conv.1.running_var\", \"encoder3.double_conv.3.weight\", \"encoder3.double_conv.4.weight\", \"encoder3.double_conv.4.bias\", \"encoder3.double_conv.4.running_mean\", \"encoder3.double_conv.4.running_var\", \"encoder4.double_conv.0.weight\", \"encoder4.double_conv.1.weight\", \"encoder4.double_conv.1.bias\", \"encoder4.double_conv.1.running_mean\", \"encoder4.double_conv.1.running_var\", \"encoder4.double_conv.3.weight\", \"encoder4.double_conv.4.weight\", \"encoder4.double_conv.4.bias\", \"encoder4.double_conv.4.running_mean\", \"encoder4.double_conv.4.running_var\", \"bottleneck.double_conv.0.weight\", \"bottleneck.double_conv.1.weight\", \"bottleneck.double_conv.1.bias\", \"bottleneck.double_conv.1.running_mean\", \"bottleneck.double_conv.1.running_var\", \"bottleneck.double_conv.3.weight\", \"bottleneck.double_conv.4.weight\", \"bottleneck.double_conv.4.bias\", \"bottleneck.double_conv.4.running_mean\", \"bottleneck.double_conv.4.running_var\", \"decoder4.double_conv.0.weight\", \"decoder4.double_conv.1.weight\", \"decoder4.double_conv.1.bias\", \"decoder4.double_conv.1.running_mean\", \"decoder4.double_conv.1.running_var\", \"decoder4.double_conv.3.weight\", \"decoder4.double_conv.4.weight\", \"decoder4.double_conv.4.bias\", \"decoder4.double_conv.4.running_mean\", \"decoder4.double_conv.4.running_var\", \"decoder3.double_conv.0.weight\", \"decoder3.double_conv.1.weight\", \"decoder3.double_conv.1.bias\", \"decoder3.double_conv.1.running_mean\", \"decoder3.double_conv.1.running_var\", \"decoder3.double_conv.3.weight\", \"decoder3.double_conv.4.weight\", \"decoder3.double_conv.4.bias\", \"decoder3.double_conv.4.running_mean\", \"decoder3.double_conv.4.running_var\", \"decoder2.double_conv.0.weight\", \"decoder2.double_conv.1.weight\", \"decoder2.double_conv.1.bias\", \"decoder2.double_conv.1.running_mean\", \"decoder2.double_conv.1.running_var\", \"decoder2.double_conv.3.weight\", \"decoder2.double_conv.4.weight\", \"decoder2.double_conv.4.bias\", \"decoder2.double_conv.4.running_mean\", \"decoder2.double_conv.4.running_var\", \"decoder1.double_conv.0.weight\", \"decoder1.double_conv.1.weight\", \"decoder1.double_conv.1.bias\", \"decoder1.double_conv.1.running_mean\", \"decoder1.double_conv.1.running_var\", \"decoder1.double_conv.3.weight\", \"decoder1.double_conv.4.weight\", \"decoder1.double_conv.4.bias\", \"decoder1.double_conv.4.running_mean\", \"decoder1.double_conv.4.running_var\". \n\tUnexpected key(s) in state_dict: \"encoder1.enc1conv1.weight\", \"encoder1.enc1norm1.weight\", \"encoder1.enc1norm1.bias\", \"encoder1.enc1norm1.running_mean\", \"encoder1.enc1norm1.running_var\", \"encoder1.enc1norm1.num_batches_tracked\", \"encoder1.enc1conv2.weight\", \"encoder1.enc1norm2.weight\", \"encoder1.enc1norm2.bias\", \"encoder1.enc1norm2.running_mean\", \"encoder1.enc1norm2.running_var\", \"encoder1.enc1norm2.num_batches_tracked\", \"encoder2.enc2conv1.weight\", \"encoder2.enc2norm1.weight\", \"encoder2.enc2norm1.bias\", \"encoder2.enc2norm1.running_mean\", \"encoder2.enc2norm1.running_var\", \"encoder2.enc2norm1.num_batches_tracked\", \"encoder2.enc2conv2.weight\", \"encoder2.enc2norm2.weight\", \"encoder2.enc2norm2.bias\", \"encoder2.enc2norm2.running_mean\", \"encoder2.enc2norm2.running_var\", \"encoder2.enc2norm2.num_batches_tracked\", \"encoder3.enc3conv1.weight\", \"encoder3.enc3norm1.weight\", \"encoder3.enc3norm1.bias\", \"encoder3.enc3norm1.running_mean\", \"encoder3.enc3norm1.running_var\", \"encoder3.enc3norm1.num_batches_tracked\", \"encoder3.enc3conv2.weight\", \"encoder3.enc3norm2.weight\", \"encoder3.enc3norm2.bias\", \"encoder3.enc3norm2.running_mean\", \"encoder3.enc3norm2.running_var\", \"encoder3.enc3norm2.num_batches_tracked\", \"encoder4.enc4conv1.weight\", \"encoder4.enc4norm1.weight\", \"encoder4.enc4norm1.bias\", \"encoder4.enc4norm1.running_mean\", \"encoder4.enc4norm1.running_var\", \"encoder4.enc4norm1.num_batches_tracked\", \"encoder4.enc4conv2.weight\", \"encoder4.enc4norm2.weight\", \"encoder4.enc4norm2.bias\", \"encoder4.enc4norm2.running_mean\", \"encoder4.enc4norm2.running_var\", \"encoder4.enc4norm2.num_batches_tracked\", \"bottleneck.bottleneckconv1.weight\", \"bottleneck.bottlenecknorm1.weight\", \"bottleneck.bottlenecknorm1.bias\", \"bottleneck.bottlenecknorm1.running_mean\", \"bottleneck.bottlenecknorm1.running_var\", \"bottleneck.bottlenecknorm1.num_batches_tracked\", \"bottleneck.bottleneckconv2.weight\", \"bottleneck.bottlenecknorm2.weight\", \"bottleneck.bottlenecknorm2.bias\", \"bottleneck.bottlenecknorm2.running_mean\", \"bottleneck.bottlenecknorm2.running_var\", \"bottleneck.bottlenecknorm2.num_batches_tracked\", \"decoder4.dec4conv1.weight\", \"decoder4.dec4norm1.weight\", \"decoder4.dec4norm1.bias\", \"decoder4.dec4norm1.running_mean\", \"decoder4.dec4norm1.running_var\", \"decoder4.dec4norm1.num_batches_tracked\", \"decoder4.dec4conv2.weight\", \"decoder4.dec4norm2.weight\", \"decoder4.dec4norm2.bias\", \"decoder4.dec4norm2.running_mean\", \"decoder4.dec4norm2.running_var\", \"decoder4.dec4norm2.num_batches_tracked\", \"decoder3.dec3conv1.weight\", \"decoder3.dec3norm1.weight\", \"decoder3.dec3norm1.bias\", \"decoder3.dec3norm1.running_mean\", \"decoder3.dec3norm1.running_var\", \"decoder3.dec3norm1.num_batches_tracked\", \"decoder3.dec3conv2.weight\", \"decoder3.dec3norm2.weight\", \"decoder3.dec3norm2.bias\", \"decoder3.dec3norm2.running_mean\", \"decoder3.dec3norm2.running_var\", \"decoder3.dec3norm2.num_batches_tracked\", \"decoder2.dec2conv1.weight\", \"decoder2.dec2norm1.weight\", \"decoder2.dec2norm1.bias\", \"decoder2.dec2norm1.running_mean\", \"decoder2.dec2norm1.running_var\", \"decoder2.dec2norm1.num_batches_tracked\", \"decoder2.dec2conv2.weight\", \"decoder2.dec2norm2.weight\", \"decoder2.dec2norm2.bias\", \"decoder2.dec2norm2.running_mean\", \"decoder2.dec2norm2.running_var\", \"decoder2.dec2norm2.num_batches_tracked\", \"decoder1.dec1conv1.weight\", \"decoder1.dec1norm1.weight\", \"decoder1.dec1norm1.bias\", \"decoder1.dec1norm1.running_mean\", \"decoder1.dec1norm1.running_var\", \"decoder1.dec1norm1.num_batches_tracked\", \"decoder1.dec1conv2.weight\", \"decoder1.dec1norm2.weight\", \"decoder1.dec1norm2.bias\", \"decoder1.dec1norm2.running_mean\", \"decoder1.dec1norm2.running_var\", \"decoder1.dec1norm2.num_batches_tracked\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-d6b0d80a4f8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m############################ NEED CHANGE FOLLOW LINE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#这里是CPU加载GPU的数据多加了参数：map_location=torch.device('cpu')，GPU加载GPU的数据无需这个参数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weights.pth\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m############################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1052\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CNNSEG:\n\tMissing key(s) in state_dict: \"encoder1.double_conv.0.weight\", \"encoder1.double_conv.1.weight\", \"encoder1.double_conv.1.bias\", \"encoder1.double_conv.1.running_mean\", \"encoder1.double_conv.1.running_var\", \"encoder1.double_conv.3.weight\", \"encoder1.double_conv.4.weight\", \"encoder1.double_conv.4.bias\", \"encoder1.double_conv.4.running_mean\", \"encoder1.double_conv.4.running_var\", \"encoder2.double_conv.0.weight\", \"encoder2.double_conv.1.weight\", \"encoder2.double_conv.1.bias\", \"encoder2.double_conv.1.running_mean\", \"encoder2.double_conv.1.running_var\", \"encoder2.double_conv.3.weight\", \"encoder2.double_conv.4.weight\", \"encoder2.double_conv.4.bias\", \"encoder2.double_conv.4.running_mean\", \"encoder2.double_conv.4.running_var\", \"encoder3.double_conv.0.weight\", \"encoder3.double_conv.1.weight\", \"encoder3.double_conv.1.bias\", \"encoder3.double_conv.1.running_mean\", \"encoder3.double_conv.1.running_var\", \"encoder3.double_conv.3.weight\", \"encoder3.double_conv.4.weight\", \"encoder3.double_conv.4.bias\", \"encoder3.double_conv.4.running_mean\", \"encoder3.double_conv.4.running_var\", \"encoder4.double_conv.0.weight\", \"encoder4.double_conv.1.weight\", \"encoder4.double_conv.1.bias\", \"encoder4.double_conv.1.running_mean\", \"encoder4.double_conv.1.running_var\", \"encoder4.double_conv.3.weight\", \"encoder4.double_conv.4.weight\", \"encoder4.double_conv.4.bias\", \"encoder4.double_conv.4.running_mean\", \"encoder4.double_conv.4.running_var\", \"bottleneck.double_conv.0.weight\", \"bottleneck.double_conv.1.weight\", \"bottleneck.double_conv.1.bias\", \"bottleneck.double_conv.1.running_mean\", \"bottleneck.double_conv.1.running_var\", \"bottleneck.double_conv.3.weight\", \"bottleneck.double_conv.4.weight\", \"bottleneck.double_conv.4.bias\", \"bottleneck.double_conv.4.running_mean\", \"bottleneck.double_conv.4.running_var\", \"decoder4.double_conv.0.weight\", \"decoder4.double_conv.1.weight\", \"decoder4.double_conv.1.bias\", \"decoder4.double_conv.1.running_mean\", \"decoder4.double_conv.1.running_var\", \"decoder4.double_conv.3.weight\", \"decoder4.double_conv.4.weight\", \"decoder4.double_conv.4.bias\", \"decoder4.double_conv.4.running_mean\", \"decoder4.double_conv.4.running_var\", \"decoder3.double_conv.0.weight\", \"decoder3.double_conv.1.weight\", \"decoder3.double_conv.1.bias\", \"decoder3.double_conv.1.running_mean\", \"decoder3.double_conv.1.running_var\", \"decoder3.double_conv.3.weight\", \"decoder3.double_conv.4.weight\", \"decoder3.double_conv.4.bias\", \"decoder3.double_conv.4.running_mean\", \"decoder3.double_conv.4.running_var\", \"decoder2.double_conv.0.weight\", \"decoder2.double_conv.1.weight\", \"decoder2.double_conv.1.bias\", \"decoder2.double_conv.1.running_mean\", \"decoder2.double_conv.1.running_var\", \"decoder2.double_conv.3.weight\", \"decoder2.double_conv.4.weight\", \"decoder2.double_conv.4.bias\", \"decoder2.double_conv.4.running_mean\", \"decoder2.double_conv.4.running_var\", \"decoder1.double_conv.0.weight\", \"decoder1.double_conv.1.weight\", \"decoder1.double_conv.1.bias\", \"decoder1.double_conv.1.running_mean\", \"decoder1.double_conv.1.running_var\", \"decoder1.double_conv.3.weight\", \"decoder1.double_conv.4.weight\", \"decoder1.double_conv.4.bias\", \"decoder1.double_conv.4.running_mean\", \"decoder1.double_conv.4.running_var\". \n\tUnexpected key(s) in state_dict: \"encoder1.enc1conv1.weight\", \"encoder1.enc1norm1.weight\", \"encoder1.enc1norm1.bias\", \"encoder1.enc1norm1.running_mean\", \"encoder1.enc1norm1.running_var\", \"encoder1.enc1norm1.num_batches_tracked\", \"encoder1.enc1conv2.weight\", \"encoder1.enc1norm2.weight\", \"encoder1.enc1norm2.bias\", \"encoder1.enc1norm2.running_mean\", \"encoder1.enc1norm2.running_var\", \"encoder1.enc1norm2.num_batches_tracked\", \"encoder2.enc2conv1.weight\", \"encoder2.enc2norm1.weight\", \"encoder2.enc2norm1.bias\", \"encoder2.enc2norm1.running_mean\", \"encoder2.enc2norm1.running_var\", \"encoder2.enc2norm1.num_batches_tracked\", \"encoder2.enc2conv2.weight\", \"encoder2.enc2norm2.weight\", \"encoder2.enc2norm2.bias\", \"encoder2.enc2norm2.running_mean\", \"encoder2.enc2norm2.running_var\", \"encoder2.enc2norm2.num_batches_tracked\", \"encoder3.enc3conv1.weight\", \"encoder3.enc3norm1.weight\", \"encoder3.enc3norm1.bias\", \"encoder3.enc3norm1.running_mean\", \"encoder3.enc3norm1.running_var\", \"encoder3.enc3norm1.num_batches_tracked\", \"encoder3.enc3conv2.weight\", \"encoder3.enc3norm2.weight\", \"encoder3.enc3norm2.bias\", \"encoder3.enc3norm2.running_mean\", \"encoder3.enc3norm2.running_var\", \"encoder3.enc3norm2.num_batches_tracked\", \"encoder4.enc4conv1.weight\", \"encoder4.enc4norm1.weight\", \"encoder4.enc4norm1.bias\", \"encoder4.enc4norm1.running_mean\", \"encoder4.enc4norm1.running_var\", \"encoder4.enc4norm1.num_batches_tracked\", \"encoder4.enc4conv2.weight\", \"encoder4.enc4norm2.weight\", \"encoder4.enc4norm2.bias\", \"encoder4.enc4norm2.running_mean\", \"encoder4.enc4norm2.running_var\", \"encoder4.enc4norm2.num_batches_tracked\", \"bottleneck.bottleneckconv1.weight\", \"bottleneck.bottlenecknorm1.weight\", \"bottleneck.bottlenecknorm1.bias\", \"bottleneck.bottlenecknorm1.running_mean\", \"bottleneck.bottlenecknorm1.running_var\", \"bottleneck.bottlenecknorm1.num_batches_tracked\", \"bottleneck.bottleneckconv2.weight\", \"bottleneck.bottlenecknorm2.weight\", \"bottleneck.bottlenecknorm2.bias\", \"bottleneck.bottlenecknorm2.running_mean\", \"bottleneck.bottlenecknorm2.running_var\", \"bottleneck.bottlenecknorm2.num_batches_tracked\", \"decoder4.dec4conv1.weight\", \"decoder4.dec4norm1.weight\", \"decoder4.dec4norm1.bias\", \"decoder4.dec4norm1.running_mean\", \"decoder4.dec4norm1.running_var\", \"decoder4.dec4norm1.num_batches_tracked\", \"decoder4.dec4conv2.weight\", \"decoder4.dec4norm2.weight\", \"decoder4.dec4norm2.bias\", \"decoder4.dec4norm2.running_mean\", \"decoder4.dec4norm2.running_var\", \"decoder4.dec4norm2.num_batches_tracked\", \"decoder3.dec3conv1.weight\", \"decoder3.dec3norm1.weight\", \"decoder3.dec3norm1.bias\", \"decoder3.dec3norm1.running_mean\", \"decoder3.dec3norm1.running_var\", \"decoder3.dec3norm1.num_batches_tracked\", \"decoder3.dec3conv2.weight\", \"decoder3.dec3norm2.weight\", \"decoder3.dec3norm2.bias\", \"decoder3.dec3norm2.running_mean\", \"decoder3.dec3norm2.running_var\", \"decoder3.dec3norm2.num_batches_tracked\", \"decoder2.dec2conv1.weight\", \"decoder2.dec2norm1.weight\", \"decoder2.dec2norm1.bias\", \"decoder2.dec2norm1.running_mean\", \"decoder2.dec2norm1.running_var\", \"decoder2.dec2norm1.num_batches_tracked\", \"decoder2.dec2conv2.weight\", \"decoder2.dec2norm2.weight\", \"decoder2.dec2norm2.bias\", \"decoder2.dec2norm2.running_mean\", \"decoder2.dec2norm2.running_var\", \"decoder2.dec2norm2.num_batches_tracked\", \"decoder1.dec1conv1.weight\", \"decoder1.dec1norm1.weight\", \"decoder1.dec1norm1.bias\", \"decoder1.dec1norm1.running_mean\", \"decoder1.dec1norm1.running_var\", \"decoder1.dec1norm1.num_batches_tracked\", \"decoder1.dec1conv2.weight\", \"decoder1.dec1norm2.weight\", \"decoder1.dec1norm2.bias\", \"decoder1.dec1norm2.running_mean\", \"decoder1.dec1norm2.running_var\", \"decoder1.dec1norm2.num_batches_tracked\". "
     ]
    }
   ],
   "source": [
    "# In this block you are expected to write code to load saved model and deploy it to all data in test set to \n",
    "# produce segmentation masks in png images valued 0,1,2,3, which will be used for the submission to Kaggle.\n",
    "data_path = './data/test'\n",
    "num_workers = 0\n",
    "batch_size = 2\n",
    "i= 121\n",
    "\n",
    "test_set = TestDataset(data_path)\n",
    "test_data_loader = DataLoader(dataset=test_set, num_workers=num_workers,batch_size=batch_size, shuffle=False)\n",
    "############################ NEED CHANGE FOLLOW LINE\n",
    "#这里是CPU加载GPU的数据多加了参数：map_location=torch.device('cpu')，GPU加载GPU的数据无需这个参数\n",
    "model.load_state_dict(torch.load(\"weights.pth\",map_location=torch.device('cpu')))\n",
    "############################\n",
    "for iteration, sample in enumerate(test_data_loader):\n",
    "    img = sample\n",
    "    img = img.to(device)\n",
    "    img = img.unsqueeze(1)\n",
    "    img = model.forward(img)\n",
    "    img = torch.argmax(img.squeeze(), dim=1)\n",
    "    #visualise all images in test set\n",
    "    img = img.data.cpu().numpy()\n",
    "    plt.imshow(img[0,...].squeeze(), cmap='gray') \n",
    "    plt.pause(1)\n",
    "    cv2.imwrite('./data/test/mask/cmr{}_mask.png'.format(i), img[0])\n",
    "    i+=1\n",
    "    cv2.imwrite('./data/test/mask/cmr{}_mask.png'.format(i), img[1])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsycVbIuUov3"
   },
   "source": [
    "## 3 Evaluation\n",
    "\n",
    "As we will automatically evaluate your predicted test makes on Kaggle, in this section we expect you to learn:\n",
    "* what is the Dice score used on Kaggle to measure your models performance\n",
    "* how to submit your predicted masks to Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NicQyj47jsD1"
   },
   "source": [
    "### 3.1 Dice Score\n",
    "\n",
    "To evaluate the quality of the predicted masks, the Dice score is adopted. Dice score on two masks A and B is defined as the intersection ratio between the overlap area and the average area of two masks. A higher Dice suggests a better registration.\n",
    "\n",
    "$Dice (A, B)= \\frac{2|A \\cap B|}{|A| + |B|} $\n",
    "\n",
    "However, in our coursework, we have three labels in each mask, we will compute the Dice score for each label and then average the three of them as the final score. Below we have given you `categorical_dice` for free so you can test your results before submission to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CNNSEG:\n\tMissing key(s) in state_dict: \"encoder1.0.weight\", \"encoder1.1.weight\", \"encoder1.1.bias\", \"encoder1.1.running_mean\", \"encoder1.1.running_var\", \"encoder1.3.weight\", \"encoder1.4.weight\", \"encoder1.4.bias\", \"encoder1.4.running_mean\", \"encoder1.4.running_var\", \"encoder2.0.weight\", \"encoder2.1.weight\", \"encoder2.1.bias\", \"encoder2.1.running_mean\", \"encoder2.1.running_var\", \"encoder2.3.weight\", \"encoder2.4.weight\", \"encoder2.4.bias\", \"encoder2.4.running_mean\", \"encoder2.4.running_var\", \"encoder3.0.weight\", \"encoder3.1.weight\", \"encoder3.1.bias\", \"encoder3.1.running_mean\", \"encoder3.1.running_var\", \"encoder3.3.weight\", \"encoder3.4.weight\", \"encoder3.4.bias\", \"encoder3.4.running_mean\", \"encoder3.4.running_var\", \"encoder4.0.weight\", \"encoder4.1.weight\", \"encoder4.1.bias\", \"encoder4.1.running_mean\", \"encoder4.1.running_var\", \"encoder4.3.weight\", \"encoder4.4.weight\", \"encoder4.4.bias\", \"encoder4.4.running_mean\", \"encoder4.4.running_var\", \"bottleneck.0.weight\", \"bottleneck.1.weight\", \"bottleneck.1.bias\", \"bottleneck.1.running_mean\", \"bottleneck.1.running_var\", \"bottleneck.3.weight\", \"bottleneck.4.weight\", \"bottleneck.4.bias\", \"bottleneck.4.running_mean\", \"bottleneck.4.running_var\", \"decoder4.0.weight\", \"decoder4.1.weight\", \"decoder4.1.bias\", \"decoder4.1.running_mean\", \"decoder4.1.running_var\", \"decoder4.3.weight\", \"decoder4.4.weight\", \"decoder4.4.bias\", \"decoder4.4.running_mean\", \"decoder4.4.running_var\", \"decoder3.0.weight\", \"decoder3.1.weight\", \"decoder3.1.bias\", \"decoder3.1.running_mean\", \"decoder3.1.running_var\", \"decoder3.3.weight\", \"decoder3.4.weight\", \"decoder3.4.bias\", \"decoder3.4.running_mean\", \"decoder3.4.running_var\", \"decoder2.0.weight\", \"decoder2.1.weight\", \"decoder2.1.bias\", \"decoder2.1.running_mean\", \"decoder2.1.running_var\", \"decoder2.3.weight\", \"decoder2.4.weight\", \"decoder2.4.bias\", \"decoder2.4.running_mean\", \"decoder2.4.running_var\", \"decoder1.0.weight\", \"decoder1.1.weight\", \"decoder1.1.bias\", \"decoder1.1.running_mean\", \"decoder1.1.running_var\", \"decoder1.3.weight\", \"decoder1.4.weight\", \"decoder1.4.bias\", \"decoder1.4.running_mean\", \"decoder1.4.running_var\". \n\tUnexpected key(s) in state_dict: \"encoder1.enc1conv1.weight\", \"encoder1.enc1norm1.weight\", \"encoder1.enc1norm1.bias\", \"encoder1.enc1norm1.running_mean\", \"encoder1.enc1norm1.running_var\", \"encoder1.enc1norm1.num_batches_tracked\", \"encoder1.enc1conv2.weight\", \"encoder1.enc1norm2.weight\", \"encoder1.enc1norm2.bias\", \"encoder1.enc1norm2.running_mean\", \"encoder1.enc1norm2.running_var\", \"encoder1.enc1norm2.num_batches_tracked\", \"encoder2.enc2conv1.weight\", \"encoder2.enc2norm1.weight\", \"encoder2.enc2norm1.bias\", \"encoder2.enc2norm1.running_mean\", \"encoder2.enc2norm1.running_var\", \"encoder2.enc2norm1.num_batches_tracked\", \"encoder2.enc2conv2.weight\", \"encoder2.enc2norm2.weight\", \"encoder2.enc2norm2.bias\", \"encoder2.enc2norm2.running_mean\", \"encoder2.enc2norm2.running_var\", \"encoder2.enc2norm2.num_batches_tracked\", \"encoder3.enc3conv1.weight\", \"encoder3.enc3norm1.weight\", \"encoder3.enc3norm1.bias\", \"encoder3.enc3norm1.running_mean\", \"encoder3.enc3norm1.running_var\", \"encoder3.enc3norm1.num_batches_tracked\", \"encoder3.enc3conv2.weight\", \"encoder3.enc3norm2.weight\", \"encoder3.enc3norm2.bias\", \"encoder3.enc3norm2.running_mean\", \"encoder3.enc3norm2.running_var\", \"encoder3.enc3norm2.num_batches_tracked\", \"encoder4.enc4conv1.weight\", \"encoder4.enc4norm1.weight\", \"encoder4.enc4norm1.bias\", \"encoder4.enc4norm1.running_mean\", \"encoder4.enc4norm1.running_var\", \"encoder4.enc4norm1.num_batches_tracked\", \"encoder4.enc4conv2.weight\", \"encoder4.enc4norm2.weight\", \"encoder4.enc4norm2.bias\", \"encoder4.enc4norm2.running_mean\", \"encoder4.enc4norm2.running_var\", \"encoder4.enc4norm2.num_batches_tracked\", \"bottleneck.bottleneckconv1.weight\", \"bottleneck.bottlenecknorm1.weight\", \"bottleneck.bottlenecknorm1.bias\", \"bottleneck.bottlenecknorm1.running_mean\", \"bottleneck.bottlenecknorm1.running_var\", \"bottleneck.bottlenecknorm1.num_batches_tracked\", \"bottleneck.bottleneckconv2.weight\", \"bottleneck.bottlenecknorm2.weight\", \"bottleneck.bottlenecknorm2.bias\", \"bottleneck.bottlenecknorm2.running_mean\", \"bottleneck.bottlenecknorm2.running_var\", \"bottleneck.bottlenecknorm2.num_batches_tracked\", \"decoder4.dec4conv1.weight\", \"decoder4.dec4norm1.weight\", \"decoder4.dec4norm1.bias\", \"decoder4.dec4norm1.running_mean\", \"decoder4.dec4norm1.running_var\", \"decoder4.dec4norm1.num_batches_tracked\", \"decoder4.dec4conv2.weight\", \"decoder4.dec4norm2.weight\", \"decoder4.dec4norm2.bias\", \"decoder4.dec4norm2.running_mean\", \"decoder4.dec4norm2.running_var\", \"decoder4.dec4norm2.num_batches_tracked\", \"decoder3.dec3conv1.weight\", \"decoder3.dec3norm1.weight\", \"decoder3.dec3norm1.bias\", \"decoder3.dec3norm1.running_mean\", \"decoder3.dec3norm1.running_var\", \"decoder3.dec3norm1.num_batches_tracked\", \"decoder3.dec3conv2.weight\", \"decoder3.dec3norm2.weight\", \"decoder3.dec3norm2.bias\", \"decoder3.dec3norm2.running_mean\", \"decoder3.dec3norm2.running_var\", \"decoder3.dec3norm2.num_batches_tracked\", \"decoder2.dec2conv1.weight\", \"decoder2.dec2norm1.weight\", \"decoder2.dec2norm1.bias\", \"decoder2.dec2norm1.running_mean\", \"decoder2.dec2norm1.running_var\", \"decoder2.dec2norm1.num_batches_tracked\", \"decoder2.dec2conv2.weight\", \"decoder2.dec2norm2.weight\", \"decoder2.dec2norm2.bias\", \"decoder2.dec2norm2.running_mean\", \"decoder2.dec2norm2.running_var\", \"decoder2.dec2norm2.num_batches_tracked\", \"decoder1.dec1conv1.weight\", \"decoder1.dec1norm1.weight\", \"decoder1.dec1norm1.bias\", \"decoder1.dec1norm1.running_mean\", \"decoder1.dec1norm1.running_var\", \"decoder1.dec1norm1.num_batches_tracked\", \"decoder1.dec1conv2.weight\", \"decoder1.dec1norm2.weight\", \"decoder1.dec1norm2.bias\", \"decoder1.dec1norm2.running_mean\", \"decoder1.dec1norm2.running_var\", \"decoder1.dec1norm2.num_batches_tracked\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-418ca488f037>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#这里是CPU加载GPU的数据多加了参数：map_location=torch.device('cpu')，GPU加载GPU的数据无需这个参数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weights.pth\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1052\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CNNSEG:\n\tMissing key(s) in state_dict: \"encoder1.0.weight\", \"encoder1.1.weight\", \"encoder1.1.bias\", \"encoder1.1.running_mean\", \"encoder1.1.running_var\", \"encoder1.3.weight\", \"encoder1.4.weight\", \"encoder1.4.bias\", \"encoder1.4.running_mean\", \"encoder1.4.running_var\", \"encoder2.0.weight\", \"encoder2.1.weight\", \"encoder2.1.bias\", \"encoder2.1.running_mean\", \"encoder2.1.running_var\", \"encoder2.3.weight\", \"encoder2.4.weight\", \"encoder2.4.bias\", \"encoder2.4.running_mean\", \"encoder2.4.running_var\", \"encoder3.0.weight\", \"encoder3.1.weight\", \"encoder3.1.bias\", \"encoder3.1.running_mean\", \"encoder3.1.running_var\", \"encoder3.3.weight\", \"encoder3.4.weight\", \"encoder3.4.bias\", \"encoder3.4.running_mean\", \"encoder3.4.running_var\", \"encoder4.0.weight\", \"encoder4.1.weight\", \"encoder4.1.bias\", \"encoder4.1.running_mean\", \"encoder4.1.running_var\", \"encoder4.3.weight\", \"encoder4.4.weight\", \"encoder4.4.bias\", \"encoder4.4.running_mean\", \"encoder4.4.running_var\", \"bottleneck.0.weight\", \"bottleneck.1.weight\", \"bottleneck.1.bias\", \"bottleneck.1.running_mean\", \"bottleneck.1.running_var\", \"bottleneck.3.weight\", \"bottleneck.4.weight\", \"bottleneck.4.bias\", \"bottleneck.4.running_mean\", \"bottleneck.4.running_var\", \"decoder4.0.weight\", \"decoder4.1.weight\", \"decoder4.1.bias\", \"decoder4.1.running_mean\", \"decoder4.1.running_var\", \"decoder4.3.weight\", \"decoder4.4.weight\", \"decoder4.4.bias\", \"decoder4.4.running_mean\", \"decoder4.4.running_var\", \"decoder3.0.weight\", \"decoder3.1.weight\", \"decoder3.1.bias\", \"decoder3.1.running_mean\", \"decoder3.1.running_var\", \"decoder3.3.weight\", \"decoder3.4.weight\", \"decoder3.4.bias\", \"decoder3.4.running_mean\", \"decoder3.4.running_var\", \"decoder2.0.weight\", \"decoder2.1.weight\", \"decoder2.1.bias\", \"decoder2.1.running_mean\", \"decoder2.1.running_var\", \"decoder2.3.weight\", \"decoder2.4.weight\", \"decoder2.4.bias\", \"decoder2.4.running_mean\", \"decoder2.4.running_var\", \"decoder1.0.weight\", \"decoder1.1.weight\", \"decoder1.1.bias\", \"decoder1.1.running_mean\", \"decoder1.1.running_var\", \"decoder1.3.weight\", \"decoder1.4.weight\", \"decoder1.4.bias\", \"decoder1.4.running_mean\", \"decoder1.4.running_var\". \n\tUnexpected key(s) in state_dict: \"encoder1.enc1conv1.weight\", \"encoder1.enc1norm1.weight\", \"encoder1.enc1norm1.bias\", \"encoder1.enc1norm1.running_mean\", \"encoder1.enc1norm1.running_var\", \"encoder1.enc1norm1.num_batches_tracked\", \"encoder1.enc1conv2.weight\", \"encoder1.enc1norm2.weight\", \"encoder1.enc1norm2.bias\", \"encoder1.enc1norm2.running_mean\", \"encoder1.enc1norm2.running_var\", \"encoder1.enc1norm2.num_batches_tracked\", \"encoder2.enc2conv1.weight\", \"encoder2.enc2norm1.weight\", \"encoder2.enc2norm1.bias\", \"encoder2.enc2norm1.running_mean\", \"encoder2.enc2norm1.running_var\", \"encoder2.enc2norm1.num_batches_tracked\", \"encoder2.enc2conv2.weight\", \"encoder2.enc2norm2.weight\", \"encoder2.enc2norm2.bias\", \"encoder2.enc2norm2.running_mean\", \"encoder2.enc2norm2.running_var\", \"encoder2.enc2norm2.num_batches_tracked\", \"encoder3.enc3conv1.weight\", \"encoder3.enc3norm1.weight\", \"encoder3.enc3norm1.bias\", \"encoder3.enc3norm1.running_mean\", \"encoder3.enc3norm1.running_var\", \"encoder3.enc3norm1.num_batches_tracked\", \"encoder3.enc3conv2.weight\", \"encoder3.enc3norm2.weight\", \"encoder3.enc3norm2.bias\", \"encoder3.enc3norm2.running_mean\", \"encoder3.enc3norm2.running_var\", \"encoder3.enc3norm2.num_batches_tracked\", \"encoder4.enc4conv1.weight\", \"encoder4.enc4norm1.weight\", \"encoder4.enc4norm1.bias\", \"encoder4.enc4norm1.running_mean\", \"encoder4.enc4norm1.running_var\", \"encoder4.enc4norm1.num_batches_tracked\", \"encoder4.enc4conv2.weight\", \"encoder4.enc4norm2.weight\", \"encoder4.enc4norm2.bias\", \"encoder4.enc4norm2.running_mean\", \"encoder4.enc4norm2.running_var\", \"encoder4.enc4norm2.num_batches_tracked\", \"bottleneck.bottleneckconv1.weight\", \"bottleneck.bottlenecknorm1.weight\", \"bottleneck.bottlenecknorm1.bias\", \"bottleneck.bottlenecknorm1.running_mean\", \"bottleneck.bottlenecknorm1.running_var\", \"bottleneck.bottlenecknorm1.num_batches_tracked\", \"bottleneck.bottleneckconv2.weight\", \"bottleneck.bottlenecknorm2.weight\", \"bottleneck.bottlenecknorm2.bias\", \"bottleneck.bottlenecknorm2.running_mean\", \"bottleneck.bottlenecknorm2.running_var\", \"bottleneck.bottlenecknorm2.num_batches_tracked\", \"decoder4.dec4conv1.weight\", \"decoder4.dec4norm1.weight\", \"decoder4.dec4norm1.bias\", \"decoder4.dec4norm1.running_mean\", \"decoder4.dec4norm1.running_var\", \"decoder4.dec4norm1.num_batches_tracked\", \"decoder4.dec4conv2.weight\", \"decoder4.dec4norm2.weight\", \"decoder4.dec4norm2.bias\", \"decoder4.dec4norm2.running_mean\", \"decoder4.dec4norm2.running_var\", \"decoder4.dec4norm2.num_batches_tracked\", \"decoder3.dec3conv1.weight\", \"decoder3.dec3norm1.weight\", \"decoder3.dec3norm1.bias\", \"decoder3.dec3norm1.running_mean\", \"decoder3.dec3norm1.running_var\", \"decoder3.dec3norm1.num_batches_tracked\", \"decoder3.dec3conv2.weight\", \"decoder3.dec3norm2.weight\", \"decoder3.dec3norm2.bias\", \"decoder3.dec3norm2.running_mean\", \"decoder3.dec3norm2.running_var\", \"decoder3.dec3norm2.num_batches_tracked\", \"decoder2.dec2conv1.weight\", \"decoder2.dec2norm1.weight\", \"decoder2.dec2norm1.bias\", \"decoder2.dec2norm1.running_mean\", \"decoder2.dec2norm1.running_var\", \"decoder2.dec2norm1.num_batches_tracked\", \"decoder2.dec2conv2.weight\", \"decoder2.dec2norm2.weight\", \"decoder2.dec2norm2.bias\", \"decoder2.dec2norm2.running_mean\", \"decoder2.dec2norm2.running_var\", \"decoder2.dec2norm2.num_batches_tracked\", \"decoder1.dec1conv1.weight\", \"decoder1.dec1norm1.weight\", \"decoder1.dec1norm1.bias\", \"decoder1.dec1norm1.running_mean\", \"decoder1.dec1norm1.running_var\", \"decoder1.dec1norm1.num_batches_tracked\", \"decoder1.dec1conv2.weight\", \"decoder1.dec1norm2.weight\", \"decoder1.dec1norm2.bias\", \"decoder1.dec1norm2.running_mean\", \"decoder1.dec1norm2.running_var\", \"decoder1.dec1norm2.num_batches_tracked\". "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def categorical_dice(mask1, mask2, label_class=1):\n",
    "    \"\"\"\n",
    "    Dice score of a specified class between two volumes of label masks.\n",
    "    (classes are encoded but by label class number not one-hot )\n",
    "    Note: stacks of 2D slices are considered volumes.\n",
    "\n",
    "    Args:\n",
    "        mask1: N label masks, numpy array shaped (H, W, N)\n",
    "        mask2: N label masks, numpy array shaped (H, W, N)\n",
    "        label_class: the class over which to calculate dice scores\n",
    "\n",
    "    Returns:\n",
    "        volume_dice\n",
    "    \"\"\"\n",
    "    mask1_pos = (mask1 == label_class).astype(np.float32)\n",
    "    mask2_pos = (mask2 == label_class).astype(np.float32)\n",
    "    dice = 2 * np.sum(mask1_pos * mask2_pos) / (np.sum(mask1_pos) + np.sum(mask2_pos))\n",
    "    return dice\n",
    "\n",
    "############################################### Evaluation Model ####################################################\n",
    "\n",
    "path = 'best_model.pth'\n",
    "val_data_path = './data/val'\n",
    "num_workers = 0\n",
    "batch_size = 4\n",
    "val_set = TrainDataset(val_data_path)\n",
    "val_data_loader = DataLoader(dataset=val_set, num_workers=num_workers, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#这里是CPU加载GPU的数据多加了参数：map_location=torch.device('cpu')，GPU加载GPU的数据无需这个参数\n",
    "model.load_state_dict(torch.load(path,map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "for iteration, sample in enumerate(val_data_loader):\n",
    "    val_img, val_mask = sample\n",
    "    val_img, val_mask = val_img.to(device),val_mask.to(device)\n",
    "    val_img = val_img.unsqueeze(1)\n",
    "    val_img = model(val_img)\n",
    "    val_pred = torch.argmax(val_img.squeeze(1), dim=1)\n",
    "    val_mask, val_pred = val_mask.data.cpu().numpy(), val_pred.data.cpu().numpy()\n",
    "    loss = Loss(val_pred.float(),val_mask.long())\n",
    "    print('val_loss:{:.4f}'.format(loss.item()))\n",
    "    \n",
    "    ave_accuracy1,ave_accuracy2,ave_accuracy3 = [],[],[]\n",
    "    ave_accuracy1.append(categorical_dice(np.array(val_pred), np.array(val_mask), 1))\n",
    "    ave_accuracy2.append(categorical_dice(np.array(val_pred), np.array(val_mask), 2))\n",
    "    ave_accuracy3.append(categorical_dice(np.array(val_pred), np.array(val_mask), 3))\n",
    "    \n",
    "    print('ave_accuracy for class1:{:.4f}'.format(np.mean(ave_accuracy1)))\n",
    "    print('ave_accuracy for class2:{:.4f}'.format(np.mean(ave_accuracy2)))\n",
    "    print('ave_accuracy for class3:{:.4f}'.format(np.mean(ave_accuracy3)))\n",
    "    print('ave_accuracy for ALL class:{:.4f}'.format((np.mean(ave_accuracy1)+np.mean(ave_accuracy2)+np.mean(ave_accuracy3))/3))\n",
    "    print('--'*10)\n",
    "    \n",
    "    #visualise all images in test set\n",
    "    #plt.imshow(val_pred[0,...].squeeze(), cmap='gray') \n",
    "    #plt.pause(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZcsrwmVjy5k"
   },
   "source": [
    "### 3.2 Submission\n",
    "\n",
    "Kaggle requires your submission to be in a specific CSV format. To help ensure your submissions are in the correct format, we have provided some helper functions to do this for you. For those interested, the png images are run-length encoded and saved in a CSV to the specifications required by our competition.\n",
    "\n",
    "It is sufficient to use this helper function. To do so, save your 80 predicted masks into a directory. ONLY the 80 predicted masks should be in this directory. Call the submission_converter function with the first argument as the directory containing your masks, and the second the directory in which you wish to save your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "uHDVbgu0qW_V"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "def rle_encoding(x):\n",
    "    '''\n",
    "    *** Credit to https://www.kaggle.com/rakhlin/fast-run-length-encoding-python ***\n",
    "    x: numpy array of shape (height, width), 1 - mask, 0 - background\n",
    "    Returns run length as list\n",
    "    '''\n",
    "    dots = np.where(x.T.flatten() == 1)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b > prev + 1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "\n",
    "def submission_converter(mask_directory, path_to_save):\n",
    "    writer = open(os.path.join(path_to_save, \"submission.csv\"), 'w')\n",
    "    writer.write('id,encoding\\n')\n",
    "\n",
    "    files = os.listdir(mask_directory)\n",
    "\n",
    "    for file in files:\n",
    "        name = file[:-4]\n",
    "        mask = cv2.imread(os.path.join(mask_directory, file), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        mask1 = (mask == 1)\n",
    "        mask2 = (mask == 2)\n",
    "        mask3 = (mask == 3)\n",
    "\n",
    "        encoded_mask1 = rle_encoding(mask1)\n",
    "        encoded_mask1 = ' '.join(str(e) for e in encoded_mask1)\n",
    "        encoded_mask2 = rle_encoding(mask2)\n",
    "        encoded_mask2 = ' '.join(str(e) for e in encoded_mask2)\n",
    "        encoded_mask3 = rle_encoding(mask3)\n",
    "        encoded_mask3 = ' '.join(str(e) for e in encoded_mask3)\n",
    "\n",
    "        writer.write(name + '1,' + encoded_mask1 + \"\\n\")\n",
    "        writer.write(name + '2,' + encoded_mask2 + \"\\n\")\n",
    "        writer.write(name + '3,' + encoded_mask3 + \"\\n\")\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "-bOn_j_FqW_V"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-71c8a40cef90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmask_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/test/mask'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath_to_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/test/submission'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msubmission_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_save\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-3858857254f1>\u001b[0m in \u001b[0;36msubmission_converter\u001b[0;34m(mask_directory, path_to_save)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mmask3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mencoded_mask1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrle_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mencoded_mask1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoded_mask1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mencoded_mask2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrle_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-3858857254f1>\u001b[0m in \u001b[0;36mrle_encoding\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[0mrun\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     '''\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mrun_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "mask_dir = './data/test/mask'\n",
    "path_to_save = './data/test/submission'\n",
    "submission_converter(mask_dir, path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CW2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
